{
    "name": "faster-whisper",
    "initial_date": "2023-11-25",
    "github_url": "https://github.com/linuxserver/docker-faster-whisper",
    "project_url": "https://github.com/SYSTRAN/faster-whisper",
    "project_logo": "",
    "description": "Reimplementation of OpenAI's Whisper model using CTranslate2, which is a fast inference engine for Transformer models. This container provides a Wyoming protocol server for faster-whisper.",
    "version": "v2.4.0-ls75",
    "version_timestamp": "2025-03-23 06:47:39+00:00",
    "category": "Machine Learning",
    "stable": true,
    "deprecated": false,
    "stars": 145,
    "monthly_pulls": 31236,
    "tags": [
        {
            "tag": "latest",
            "desc": "Stable releases"
        },
        {
            "tag": "gpu",
            "desc": "Releases with Nvidia GPU support (amd64 only)"
        }
    ],
    "architectures": [
        {
            "arch": "x86_64",
            "tag": "amd64-latest"
        },
        {
            "arch": "arm64",
            "tag": "arm64v8-latest"
        }
    ],
    "changelog": [
        {
            "date": "2024-12-30",
            "desc": "Add arm64 support for non-GPU builds."
        },
        {
            "date": "2024-12-05",
            "desc": "Build from Github releases rather than Pypi."
        },
        {
            "date": "2024-07-18",
            "desc": "Rebase to Ubuntu Noble."
        }
    ],
    "config": {
        "application_setup": "https://github.com/linuxserver/docker-faster-whisper?tab=readme-ov-file#application-setup",
        "readonly_supported": true,
        "env_vars": [
            {
                "name": "PUID",
                "value": "1000",
                "desc": "User ID",
                "optional": false
            },
            {
                "name": "PGID",
                "value": "1000",
                "desc": "Group ID",
                "optional": false
            },
            {
                "name": "TZ",
                "value": "Etc/UTC",
                "desc": "Timezone",
                "optional": false
            },
            {
                "name": "WHISPER_MODEL",
                "value": "tiny-int8",
                "desc": "Whisper model that will be used for transcription. From [here](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/utils.py#L12-L31), all with `-int8` compressed variants",
                "optional": false
            },
            {
                "name": "WHISPER_BEAM",
                "value": "1",
                "desc": "Number of candidates to consider simultaneously during transcription.",
                "optional": true
            },
            {
                "name": "WHISPER_LANG",
                "value": "en",
                "desc": "Language that you will speak to the add-on.",
                "optional": true
            }
        ],
        "volumes": [
            {
                "path": "/config",
                "host_path": "/path/to/faster-whisper/data",
                "desc": "Local path for Whisper config files.",
                "optional": false
            }
        ],
        "ports": [
            {
                "external": "10300",
                "internal": "10300",
                "desc": "Wyoming connection port.",
                "optional": false
            }
        ]
    }
}
